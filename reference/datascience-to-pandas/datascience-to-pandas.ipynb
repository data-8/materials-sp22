{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-0",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datascience import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cell-datascience-to-pandas-1",
   "metadata": {},
   "source": [
    "## Conversion Notebook: From `datascience` library to Python's `pandas` library"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cell-datascience-to-pandas-2",
   "metadata": {},
   "source": [
    "Throughout this course, we have been working a lot with the `datascience` library, a library created by faculty at UC Berkeley, specifically for this course. While this library is not used outside of this course, all of the ideas and concepts behind the library and the different functions are definitely used when dealing with data science problems in the real world. One of the common libraries used in industry is called `pandas`, and is a way to structure and analyze rectangular/tabular data. Using the `datascience` library in Data 8X is a solid stepping stone to understanding `pandas` better. Throughout this notebook, we will be going over certain concepts that we saw in the `datascience` library and showing the equivalent functions that we will use in `pandas`. The syntax and function names may be different but the underlying concepts are still the same! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-datascience-to-pandas-3",
   "metadata": {},
   "source": [
    "Above, we `import pandas as pd`, which means that any function associated with pandas should be called using `pd.function_name()`. This tells Python that we want to use the specific `function_name` from the Pandas library. In theory, we could do `import pandas as pandas` or any other name, but it is known and commonly used to import it as `pd`. We will see some examples of this later in this notebook.\n",
    "\n",
    "For reference:\n",
    "\n",
    "Datascience documentation: http://data8.org/datascience/index.html\n",
    "\n",
    "Python Reference: http://data8.org/python-reference/python-reference.html\n",
    "\n",
    "Pandas documentation: https://pandas.pydata.org/pandas-docs/stable/dsintro.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-datascience-to-pandas-4",
   "metadata": {},
   "source": [
    "### Tables and DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-datascience-to-pandas-5",
   "metadata": {},
   "source": [
    "In `datascience`, we have something called a Table, which is a way to organize your data in a tabular format, which makes accessing rows and columns of data easier. In `pandas`, this structure is called a DataFrame. A DataFrame is the primary data structure in `pandas`. Similar to a Table, we can access different rows and columns of a DataFrame. Tables are essentially the same as DataFrames: we can do similar actions and functions with both.\n",
    "\n",
    "In the following lines of code, we create a Table and DataFrame of data by importing and reading an external [csv file](https://en.wikipedia.org/wiki/Comma-separated_values). The Table will be called `cones_table` and the DataFrame will be called `cones_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-6",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# datascience\n",
    "cones_table = Table.read_table('cones.csv') # read the cones csv file into the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-7",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# pandas\n",
    "cones_df = pd.read_csv('cones.csv') # read the cones csv file into the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cones_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cones_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-datascience-to-pandas-10",
   "metadata": {},
   "source": [
    "We see here how to import an external csv file into our current notebook using both libraries. With `pandas` `read_csv()` function, we use `pd.read_csv()` to let Python know we want to use this particular `pandas` function.\n",
    "Notice how there is an extra column of numbers in the DataFrame that results. These numbers are called the index of a DataFrame, and will be useful later in order to select different rows that we are interested in. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-datascience-to-pandas-11",
   "metadata": {},
   "source": [
    "In `pandas`, we have two types of objects: DataFrames and Series. Series are similar to a DataFrame except there is only one column in a Series. You can think of Series as a single column of a DataFrame. Later in this notebook, we will use Series as well as DataFrames. Series come up when we select one column of a DataFrame, apply different functions to a column, etc.\n",
    "\n",
    "You can learn more about DataFrames and Series with this [documentation](https://pandas.pydata.org/pandas-docs/stable/dsintro.html) as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-datascience-to-pandas-12",
   "metadata": {},
   "source": [
    "### Make a Table/DataFrame\n",
    "If we want to create a new Table or DataFrame from scratch, we can call the respective functions and then specify the columns and the data within each column (where the data is in array/list format). Some examples from both `datascience` and `pandas` are shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datascience\n",
    "flowers = Table().with_columns(\n",
    "    'Number of petals', make_array(8, 34, 5),\n",
    "    'Name', make_array('lotus', 'sunflower', 'rose')\n",
    ") # create Table with 2 columns and 3 rows\n",
    "flowers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas\n",
    "flowers1 = pd.DataFrame(data = {'Number of petals': [8, 34, 5], 'Name': ['lotus', 'sunflower', 'rose']})\n",
    "flowers1 # create DataFrame with 2 columns and 3 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-datascience-to-pandas-15",
   "metadata": {},
   "source": [
    "### Select Columns\n",
    "In both `datascience` and `pandas` we can select a column or multiple columns, based on what information we want from the data. In `datascience` we use the `select()` function, while in `pandas` we use indexing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datascience\n",
    "# select one column\n",
    "cones_table.select(\"Color\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datascience\n",
    "# select multiple columns\n",
    "cones_table.select(\"Flavor\", \"Color\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas\n",
    "# select one column - return Series\n",
    "cones_df[\"Color\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas\n",
    "# select one column - return DataFrame\n",
    "cones_df[[\"Color\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas\n",
    "# select a specific column\n",
    "cones_df.Color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas\n",
    "# select multiple columns\n",
    "cones_df[[\"Flavor\", \"Color\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-datascience-to-pandas-22",
   "metadata": {},
   "source": [
    "We can also use `loc` and `iloc` to select specific columns, which we will introduce in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-datascience-to-pandas-23",
   "metadata": {},
   "source": [
    "### Select Rows\n",
    "In `datascience`, we use `take` in order to select certain rows, based on what row numbers we want to select (0 indexed). With selecting multiple rows, we use the concept of list slicing in order to select a sequence of rows, or even select multiple rows (if the numbers are in an array). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datascience \n",
    "cones_table.take(2) # select the row with an index of 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-25",
   "metadata": {},
   "outputs": [],
   "source": [
    "cones_table.take(np.arange(1, 3)) # select the rows with index from 1 to 3 (not 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-26",
   "metadata": {},
   "outputs": [],
   "source": [
    "cones_table.take[0, 3, 4] # select the rows with index 0, 3, 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas\n",
    "cones_df[0:3] # select the rows with index from 0 to 3 (not including 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-datascience-to-pandas-28",
   "metadata": {},
   "source": [
    "#### loc and iloc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-datascience-to-pandas-29",
   "metadata": {},
   "source": [
    "With `pandas`, we can use `loc` and `iloc` to select certain rows or columns from a DataFrame - all at once - making it a powerful tool. `loc` gets rows or columns with particular labels from the index. `iloc` gets rows or columns at particular positions in the index (so it only takes integers). With both of these, we index using brackets and specify which rows and columns we want to select based on `[row, column]`. Note: if we have just `:` for either row or column, this means that we select all of it (based on if it is in the row or column section of the brackets). An example is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-30",
   "metadata": {},
   "outputs": [],
   "source": [
    "cones_df.loc[:,'Flavor'] # select all rows but only the column Flavor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-31",
   "metadata": {},
   "outputs": [],
   "source": [
    "cones_df.loc[:,['Flavor', 'Color']] # select all rows but only the Flavor and Color columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-32",
   "metadata": {},
   "outputs": [],
   "source": [
    "cones_df.iloc[1:3,0:3] # select the rows with index from 1 to 3 (not including 3) and columns in the positions of 0 to 3 (not including 3) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-datascience-to-pandas-33",
   "metadata": {},
   "source": [
    "### Rename Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-datascience-to-pandas-34",
   "metadata": {},
   "source": [
    "In the `datascience` package, we can rename a column or multiple columns. We may want to do this when we update a column to make the title more specific. We also need to make sure that the new column name is not the same as any other existing columns (so we do not have two columns with the same name as this will error). We do this with the `relabeled()` function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-35",
   "metadata": {},
   "outputs": [],
   "source": [
    "cones_table.relabeled('Price', 'New Price') # rename the Price columns as New Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-36",
   "metadata": {},
   "outputs": [],
   "source": [
    "cones_table.relabeled(['Color', 'Price'], ['Ice Cream Color', 'New Price']) # rename two columns: Color as Ice Cream Color and Price as New Price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-datascience-to-pandas-37",
   "metadata": {},
   "source": [
    "In `pandas` we can use a similar function called `rename()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-38",
   "metadata": {},
   "outputs": [],
   "source": [
    "cones_df.rename(columns={'Flavor': 'New Flavor'}) # rename the Flavor column as New Flavor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-39",
   "metadata": {},
   "outputs": [],
   "source": [
    "cones_df.rename(columns={'Flavor': 'New Flavor', 'Color': 'New Color'}) # rename Flavor as New Flavor and Color as New Color"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-datascience-to-pandas-40",
   "metadata": {},
   "source": [
    "### Where/Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-datascience-to-pandas-41",
   "metadata": {},
   "source": [
    "In `datascience` there is a function called `where` which creates a copy of a table with only the rows that match some condition. To filter out a DataFrame by its contents in `pandas`, we need to use boolean (`True` or `False`) expressions in order to select the rows we want to keep."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datascience\n",
    "cones_table.where(\"Price\", are.above(4)) # select/filter rows so the Price is above 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-datascience-to-pandas-43",
   "metadata": {},
   "source": [
    "We want to keep the rows that have a price that is greater than 4. We can first create a boolean array, which will assign a `True` or `False` value to each row based on if it satisfies the condition we give it. We can then index this array into our original array and only the values that are `True` will be returned in a DataFrame.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas\n",
    "boolean_array = cones_df[\"Price\"] > 4 # create a boolean array saying if the Price is greater than 4\n",
    "boolean_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-45",
   "metadata": {},
   "outputs": [],
   "source": [
    "cones_df[boolean_array] # apply boolean array to DataFrame and filter the rows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-datascience-to-pandas-46",
   "metadata": {},
   "source": [
    "We can also do this all in one step as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-47",
   "metadata": {},
   "outputs": [],
   "source": [
    "cones_df[cones_df[\"Price\"] > 4] # filter the DataFrame based on Price greater than 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-datascience-to-pandas-48",
   "metadata": {},
   "source": [
    "Sometimes we want to filter by multiple different conditions. In `pandas` we can do this using parentheses and a symbol indicating if we want both conditions to be satisfied (and) or at least one to be satisfied (or). The format is: \n",
    "\n",
    "`df[(condition1) & (condition2)]`\n",
    "\n",
    "or \n",
    "\n",
    "`df[(condition1) | (condition2)]`\n",
    "\n",
    "Note: `&` is different from `and` \n",
    "also `|` is different from `or`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-49",
   "metadata": {},
   "outputs": [],
   "source": [
    "cones_df[(cones_df[\"Price\"] > 4) & (cones_df[\"Price\"] < 5)] # filter the DataFrame based on Price greater than 4 and less than 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-datascience-to-pandas-50",
   "metadata": {},
   "source": [
    "### Sort, Group, Pivot\n",
    "#### Sort\n",
    "We can sort values similarly in Tables and DataFrames - using a function and what column we want to sort by."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datascience\n",
    "cones_table.sort('Price') # sort the table by Price column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas\n",
    "cones_df.sort_values('Price') # sort DataFrame by Price column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-datascience-to-pandas-53",
   "metadata": {},
   "source": [
    "If we want to specify whether we sort in ascending order or descending order. In `datascience` we can do this with the `descending` parameter, and sending it equal to `True` or `False`. In `pandas` we can use the `ascending` parameter and set this equal to `True` or `False`. By default, the column sorts in ascending order.\n",
    "\n",
    "#### Group \n",
    "In both `datascience` and `pandas` we can use group functions that allow us to group records of our data into buckets. You can think of group by as splitting the dataset data into buckets. Then you can call \"aggregate\" functions (`mean`, `sum`, `max`, `min`, etc) on these buckets to find these values per bucket (which can lead to interesting analysis)!\n",
    "\n",
    "Let's say that we want to group by flavor of ice cream and see what the total sum is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datascience\n",
    "cones_table.select(['Flavor', 'Price']).group('Flavor', collect=sum) # select the Flavor and Price columns and then group by Flavor and find the sum price per flavor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas\n",
    "cones_df[['Flavor', 'Price']].groupby('Flavor').sum() # select the Flavor and Price columns and then group by Flavor and find the sum price per flavor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-datascience-to-pandas-56",
   "metadata": {},
   "source": [
    "#### Pivot\n",
    "We can create pivot tables in both `datascience` and `pandas` using different functions and specifying columns, index, values, and the collect/aggregate function acting on the values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datascience\n",
    "cones_table.pivot(columns = 'Flavor', rows = 'Color', values = 'Price', collect=sum) # create a pivot table with Flavors and Color and sum prices for corresponding entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas\n",
    "cones_df.pivot_table(values='Price', index=['Color'], columns=['Flavor'], aggfunc=sum) # create a pivot table with Flavors and Color and sum prices for corresponding entries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-datascience-to-pandas-59",
   "metadata": {},
   "source": [
    "Note: We have `NaN` as values in our above table because `pandas` cannot find appropriate values for those specific combinations of rows and columns. If we want to replace these values with 0, we can use `fillna(0)` on the resulting pivot table. \n",
    "\n",
    "Don't be alarmed by these `NaN` values! This is something to note about real data, often times the data is not cleaned already and null values are very common. They may even be important in your exploration of the data, the number of null values you have and where they occur could be important!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-datascience-to-pandas-60",
   "metadata": {},
   "source": [
    "### Visualizing Data\n",
    "For the purpose of representing and showing how to plot different visualizations, we will now create another Table/DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-61",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data_table = Table().with_columns(['data1', np.random.normal(loc = 1, scale = 2, size = 100), 'data2', np.random.normal(loc = 4, scale = 3, size = 100)])\n",
    "normal_data_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-62",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_data_df = pd.DataFrame(data = {'data1': np.random.normal(loc = 1, scale = 2, size = 100), 'data2': np.random.normal(loc = 4, scale = 3, size = 100)})\n",
    "normal_data_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-datascience-to-pandas-63",
   "metadata": {},
   "source": [
    "#### Histograms\n",
    "`datascience` histograms: http://data8.org/datascience/_autosummary/datascience.tables.Table.hist.html\n",
    "\n",
    "`pandas` histograms: https://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.DataFrame.hist.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datascience\n",
    "new = normal_data_table.select('data1') # create histogram of data1\n",
    "new.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas\n",
    "new = normal_data_df[[\"data1\"]] # create histogram of data1\n",
    "new.hist();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-datascience-to-pandas-66",
   "metadata": {},
   "source": [
    "#### Scatterplots\n",
    "`datascience` scatterplots: http://data8.org/datascience/_autosummary/datascience.tables.Table.scatter.html\n",
    "\n",
    "`pandas` scatterplots: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.plot.scatter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datascience\n",
    "normal_data_table.scatter('data1') # create scatterplot of data1 vs data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas\n",
    "normal_data_df.plot.scatter(x = 'data1', y = 'data2'); # create scatterplot of data1 vs data2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-datascience-to-pandas-69",
   "metadata": {},
   "source": [
    "#### Barplots\n",
    "`datascience` barplots: http://data8.org/datascience/_autosummary/datascience.tables.Table.barh.html\n",
    "\n",
    "`pandas` barplots: https://pandas.pydata.org/pandas-docs/version/0.23/generated/pandas.DataFrame.plot.bar.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datascience\n",
    "t = Table().with_columns('Furniture', make_array('chairs', 'tables', 'desks'), 'Count', make_array(6, 1, 2), 'Price', make_array(10, 20, 30)) # barplot of count and price of different furnitures\n",
    "t.barh('Furniture')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas\n",
    "df = pd.DataFrame(data = {'Furniture': ['chairs', 'tables', 'desks'], 'Count': [6, 1, 2], 'Price': [10, 20, 30]})\n",
    "df.plot.barh('Furniture'); # barplot of count and price of different furnitures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-datascience-to-pandas-72",
   "metadata": {},
   "source": [
    "For all of the plots above, there are many different parameters you can play around with (color, size, orientation, figure size, axis labels, title, etc). Above are the basic implementations of these graphs but we recommend looking through the documentation and seeing how you can change different aspects of it. Try using the DataFrame given and plotting a different graph!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-datascience-to-pandas-73",
   "metadata": {},
   "source": [
    "### Export to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-datascience-to-pandas-74",
   "metadata": {},
   "source": [
    "When we want to convert a Table to a csv file, we first need to convert it to a DataFrame and then to a csv file. In `pandas`, we can get rid of this intermediate step because we already have a DataFrame!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-75",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cones_table.to_df().to_csv('cones_datascience.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-76",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cones_df.to_csv('cones_dataframe.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-datascience-to-pandas-77",
   "metadata": {},
   "source": [
    "### Calculate the number of columns\n",
    "In `datascience`, calculating the number of columns (or rows) requires a simple call using our Table, either `num_columns` or `num_rows`. With `pandas`, there are two ways to get the number of columns and rows. The `len` function can be used on a part of the DataFrame, or the `shape` function can be used. The `shape` function returns both the number of rows and the number of columns so based on what we want, we will have to select it using indexing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datascience\n",
    "cones_table.num_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas\n",
    "len(cones_df.columns) # find the length of the list of column names\n",
    "# or\n",
    "cones_df.shape[1] # select the column part of shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-datascience-to-pandas-80",
   "metadata": {},
   "source": [
    "### Calculate the number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datascience\n",
    "cones_table.num_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas\n",
    "len(cones_df) # number of rows in the DataFrame\n",
    "# or\n",
    "cones_df.shape[0] # select the row part of shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-datascience-to-pandas-83",
   "metadata": {},
   "source": [
    "### Apply\n",
    "In both libraries, we have a function called `apply`, which we use to apply a function on a certain column and all its elements. Note: in `pandas`, apply works on a Series (since this is essentially a single column of a DataFrame)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datascience\n",
    "cones_table.apply(round, \"Price\") # apply the round function to the column Price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas\n",
    "cones_df[\"Price\"].apply(round, \"Price\") # apply the round function to the column Price"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-datascience-to-pandas-86",
   "metadata": {},
   "source": [
    "Above we see that we get the rounded up prices returned to us. If we wanted to change the column in the original Table/DataFrame, we would set the expression above equal to the column/series so that the change is made and saved in the original.\n",
    "\n",
    "For example:\n",
    "\n",
    "`\n",
    "cones_table = cones_table.apply(round, \"Price\")\n",
    "`\n",
    "\n",
    "or\n",
    "\n",
    "`\n",
    "cones_df[\"Price\"] = cones_df[\"Price\"].apply(round, \"Price\")\n",
    "`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-datascience-to-pandas-87",
   "metadata": {},
   "source": [
    "### Joining\n",
    "Joins are useful when we want to combine two or more tables together, so we can do analysis on all of the tables. In `datascience`, we can use `join` and in `pandas` we can use `merge`. With both we need the information of the two tables and how we are joining on them (the appropriate columns). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datascience\n",
    "table = Table().with_columns('first', make_array('i', 'c', 'c', 'a'), 'second', make_array('a', 'b', 'b', 'j'), 'third', make_array('c', 'd', 'e', 'f'))\n",
    "table2 = Table().with_columns( 'another', make_array('i', 'a', 'a', 'a'), 'fourth', make_array('a', 'b', 'b', 'j'), 'fifth', make_array('c', 'd', 'e', 'f'))\n",
    "print(table)\n",
    "print()\n",
    "print(table2)\n",
    "table.join('first', table2, 'fourth') # join table and table 2 together so columns first and fourth match values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-datascience-to-pandas-89",
   "metadata": {},
   "source": [
    "In the above code block, we create two different tables. When we call the `join()` function on these two tables, we take the cross product of the rows of both tables (every combination of rows that could happen between both tables) and then filter this out based on the columns we specify that need to be the same. In this case we have row `first` from the first table and row `fourth` from the second table, therefore we can only keep rows that have a value in `first` from the first table and `fourth` from the second table that are the same. In our example, the similar value is `first` and `fourth` is the value `a` (we see it in the last row of the `first` column and the first row of the `fourth` column). We then look at the values in these two rows and create a table with the values. Similarly, we can replicate this in `pandas`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-datascience-to-pandas-90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data = {'first': ['i', 'c', 'c', 'a'], 'second': ['a', 'b', 'b', 'j'], 'third': ['c', 'd', 'e', 'f']})\n",
    "df2 = pd.DataFrame(data = {'another': ['i', 'a', 'a', 'a'], 'fourth': ['a', 'b', 'b', 'j'], 'fifth': ['c', 'd', 'e', 'f']})\n",
    "print(df)\n",
    "print()\n",
    "print(df2)\n",
    "df.merge(df2, left_on = 'first', right_on = 'fourth', how = \"inner\") # merge df and df2 using an inner join using columns first and fourth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-datascience-to-pandas-91",
   "metadata": {},
   "source": [
    "In the `pandas` code above, we have an extra argument that we need to specify: how. There are many types of joins that we can do with data. In `datascience`, the only option we can do is an inner join, but in `pandas`, we have the option to do inner, left, right, outer joins. For more information about these types of joins, check out this page: http://pandas.pydata.org/pandas-docs/version/0.19.1/generated/pandas.DataFrame.merge.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-datascience-to-pandas-92",
   "metadata": {},
   "source": [
    "### Reading `pandas` Documentation\n",
    "There are many more functions and methods you can call with `pandas` to do more cool things with Series and DataFrames. One way to learn more about this is by looking through the `pandas` documentation. The documentation has all the different functions associated with `pandas`, and descriptions about what they do, how you use them, and some examples. \n",
    "\n",
    "Some tips for reading through the documentation:\n",
    "* For various functions there are LOTS of different parameters that you can call, usually there are only a few that are important (usually related to the data you are working with and specifying how to run the functions). There are some parameters that are optional and you do not have to specify (automatically Pandas will use default settings for these functions). For example, for joins, an inner join is the default setting but if needed, you can say you want to do an outer join etc.\n",
    "* Another useful parameter is the `inplace` parameter. When set to true, the data is renamed/the function is run in place (a copy of the new data with the function applied is NOT created)\n",
    "* For quick lookup for a specific function: in Jupyter you can put your cursor on a Pandas function and press Shift + Tab in order to see part of the documentation. Based on how many times you press the Tab, it will show you the header of the function (how you would call it and parameters) and a description of the function overall and a description of the parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
